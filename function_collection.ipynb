{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Functions used throughout the project to "
      ],
      "metadata": {
        "id": "l4tAjHa-wGZL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zv-r_IOWNt36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NjDACOERlM9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Option to create patches. Not used currently with this second dataset. \n",
        "\n",
        "def split_image_into_patches(image, patch_size):\n",
        "    # print(image.shape)\n",
        "    height, width, _ = image.shape\n",
        "    patch_width, patch_height = patch_size\n",
        "\n",
        "    patches = []\n",
        "    for y in range(0, height, patch_height):\n",
        "        for x in range(0, width, patch_width):\n",
        "            if x + patch_width <= width and y + patch_height <= height:\n",
        "                patch = image[y:y+patch_height, x:x+patch_width].copy()\n",
        "                patches.append(patch)\n",
        "\n",
        "    return patches\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_patches(path_to_data, output_folder, dims): \n",
        "\n",
        "    image_names = sorted(os.listdir(path_to_data))\n",
        "\n",
        "    # print(image_names)\n",
        "\n",
        "    if \".DS_Store\" in image_names:\n",
        "        image_names.remove(\".DS_Store\")\n",
        "\n",
        "    if \".ipynb_checkpoints\" in image_names:\n",
        "        image_names.remove(\".ipynb_checkpoints\")\n",
        "\n",
        "    \n",
        "    i = 0 \n",
        "\n",
        "    for img in image_names:\n",
        "        # print(img)\n",
        "\n",
        "        # if re.match(r\".*(FS[1-9]_).*\", img):\n",
        "\n",
        "            # print('Pass')\n",
        "\n",
        "          actual_image = cv2.imread(path_to_data + \"/\" + img)\n",
        "\n",
        "          i_patches = split_image_into_patches(actual_image, dims)\n",
        "          print(\"Len i_patches \", len(i_patches))\n",
        "\n",
        "          for p in i_patches:\n",
        "\n",
        "              cv2.imwrite(output_folder + \"/{}.png\".format(i), p )\n",
        "              i+=1\n",
        "\n",
        "    \n",
        "    return \n",
        "\n",
        "# for img, mask in zip(image_patches, image_mask_patches):\n",
        "\n",
        "#     fig, (ax, m_ax) = plt.subplots(ncols = 2)\n",
        "\n",
        "#     ax.imshow(img * 3.5/255)\n",
        "#     ax.set_title('Image Patch')\n",
        "#     ax.tick_params(axis = 'x' , bottom = False, labelbottom = False)\n",
        "#     ax.tick_params(axis = 'y' , left = False, labelleft = False)\n",
        "\n",
        "\n",
        "#     m_ax.imshow(mask * 255)\n",
        "#     m_ax.set_title('True Mask')\n",
        "#     m_ax.tick_params(axis = 'x' , bottom = False, labelbottom = False)\n",
        "#     m_ax.tick_params(axis = 'y' , left = False, labelleft = False)\n"
      ],
      "metadata": {
        "id": "BLMqHRatlP7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Load masks\n",
        "directory = 'filtered_masks'\n",
        "mask_filenames = sorted([filename for filename in os.listdir(directory) if filename.endswith(('.png', '.jpeg', '.jpg'))])\n",
        "\n",
        "\n",
        "masks = []\n",
        "\n",
        "for filename in mask_filenames:\n",
        "\n",
        "    mask = cv2.imread(os.path.join(directory, filename), cv2.IMREAD_GRAYSCALE)\n",
        "    mask[mask == 2] = 0 ## This removes the cloud mask \n",
        "    masks.append(mask)\n",
        "\n",
        "masks_array = np.array(masks)\n",
        "\n",
        "\n",
        "## Lad images\n",
        "directory = 'filtered_images'\n",
        "img_filenames = sorted([filename for filename in os.listdir(directory) if filename.endswith(('.png', '.jpeg', '.jpg'))])\n",
        "\n",
        "\n",
        "images = []\n",
        "\n",
        "for filename in img_filenames:\n",
        "  \n",
        "    image = cv2.imread(os.path.join(directory, filename))\n",
        "    images.append(image)\n",
        "\n",
        "\n",
        "images_array = np.array(images)\n",
        "\n",
        "print(images_array.shape)\n",
        "print(masks_array.shape)"
      ],
      "metadata": {
        "id": "a9xjgKS2lS_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0 \n",
        "while i < 10: \n",
        "  i+=1 \n",
        "  # plt.imshow(masks_array[i])\n",
        "\n",
        "\n",
        "  fig, (ax, m_ax) = plt.subplots(ncols = 2)\n",
        "  ax.imshow(images_array[i])\n",
        "  ax.set_title('Image Patch')\n",
        "  ax.tick_params(axis = 'x' , bottom = False, labelbottom = False)\n",
        "  ax.tick_params(axis = 'y' , left = False, labelleft = False)\n",
        "\n",
        "\n",
        "  m_ax.imshow(masks_array[i])\n",
        "  m_ax.set_title('True Mask')\n",
        "  m_ax.tick_params(axis = 'x' , bottom = False, labelbottom = False)\n",
        "  m_ax.tick_params(axis = 'y' , left = False, labelleft = False)"
      ],
      "metadata": {
        "id": "rYNP__Y5lXO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create_patches(\"filtered_masks\", \"224_masks\", (224,224))\n",
        "\n",
        "# create_patches(\"filtered_images\", \"224_images\", (224,224))"
      ],
      "metadata": {
        "id": "QyBtm9mslZzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load masks into numpy array \n",
        "directory = \"224_masks\"\n",
        "\n",
        "mask_filenames = [filename for filename in os.listdir(directory) if filename.endswith(('.png', '.jpeg', '.jpg'))]\n",
        "\n",
        "masks = []\n",
        "fire_files = []\n",
        "\n",
        "for filename in mask_filenames:\n",
        "    mask = cv2.imread(os.path.join(directory, filename), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if np.count_nonzero(mask == 1) > 100:\n",
        "        # print(np.count_nonzero(mask))\n",
        "        masks.append(mask)\n",
        "\n",
        "        fire_files.append(filename)\n",
        "\n",
        "\n",
        "masks_array = np.array(masks)\n",
        "\n",
        "# Print the shape of the array\n",
        "print(\"Shape of the masks array:\", masks_array.shape)\n",
        "\n",
        "directory = \"224_images\"\n",
        "\n",
        "# image_filenames = [filename for filename in os.listdir(directory) if filename.endswith(('.png', '.jpeg', '.jpg'))]\n",
        "\n",
        "images = []\n",
        "\n",
        "for filename in fire_files:\n",
        "    image = cv2.imread(os.path.join(directory, filename))\n",
        "    images.append(image)\n",
        "\n",
        "images_array = np.array(images)\n",
        "\n",
        "print(\"Shape of the images array:\", images_array.shape)"
      ],
      "metadata": {
        "id": "ArMiVOu8lb6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(images_array)):\n",
        "\n",
        "#   cv2.imwrite(\"224_images_filtered/{}.png\".format(i), images_array[i])\n",
        "#   cv2.imwrite(\"224_masks_filtered/{}.png\".format(i), masks_array[i])"
      ],
      "metadata": {
        "id": "pmcKG8eTlgzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjejoJIKkhT_"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Load masks\n",
        "mask_directory = '224_masks_filtered'\n",
        "mask_filenames = [filename for filename in os.listdir(mask_directory) if filename.endswith(('.png', '.jpeg', '.jpg'))]\n",
        "\n",
        "image_directory = '224_images_filtered'\n",
        "img_filenames = [filename for filename in os.listdir(image_directory) if filename.endswith(('.png', '.jpeg', '.jpg'))]\n",
        "\n",
        "filenames = mask_filenames\n",
        "# filenames = list(set(mask_filenames) & set(img_filenames)) ## Intersection -\n",
        "# filenames = np.intersect1d(mask_filenames, img_filenames)\n",
        "# print(filenames)\n",
        "\n",
        "\n",
        "masks = []\n",
        "\n",
        "for filename in filenames:\n",
        "\n",
        "    mask = cv2.imread(os.path.join(mask_directory, filename), cv2.IMREAD_GRAYSCALE)\n",
        "    # plt.imshow(mask)\n",
        "    mask[mask == 2] = 0 ## This removes the cloud mask \n",
        "    masks.append(mask)\n",
        "\n",
        "masks_array = np.array(masks)\n",
        "\n",
        "\n",
        "## Lad images\n",
        "directory = '224_images_filtered'\n",
        "img_filenames = [filename for filename in os.listdir(image_directory) if filename.endswith(('.png', '.jpeg', '.jpg'))]\n",
        "\n",
        "\n",
        "images = []\n",
        "\n",
        "\n",
        "for filename in filenames:\n",
        "  \n",
        "    image = cv2.imread(os.path.join(image_directory, filename))\n",
        "    images.append(image)\n",
        "\n",
        "\n",
        "images_array = np.array(images)\n",
        "\n",
        "print(images_array.shape)\n",
        "print(masks_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## IF we want to use a tensorflow dataset - not used in this example\n",
        "\n",
        "\n",
        "# from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "\n",
        "# # Step 2: Convert the numpy arrays to TensorFlow tensors\n",
        "# images = tf.convert_to_tensor(images_array, dtype=tf.float32)\n",
        "# masks = tf.convert_to_tensor(masks_array, dtype=tf.float32)\n",
        "\n",
        "\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
        "\n",
        "# # Step 4: Apply any additional transformations or augmentations\n",
        "\n",
        "# # dataset = dataset.map(lambda img, mask: (img / 255.0, mask))\n",
        "\n",
        "# # Step 5: Shuffle, batch, and prefetch the dataset\n",
        "# shuffle_buffer_size = 1000\n",
        "# batch_size = 16\n",
        "# prefetch_buffer_size = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
        "# dataset = dataset.batch(batch_size)\n",
        "# dataset = dataset.prefetch(prefetch_buffer_size)\n",
        "  \n",
        "# print(dataset.cardinality().numpy())\n",
        "\n",
        "\n",
        "# # Define the size of each split (percentage of the original dataset)\n",
        "# train_size = 0.7  # 80% for training\n",
        "# val_size = 0.2    # 10% for validation\n",
        "# test_size = 0.1   # 10% for testing\n",
        "\n",
        "# # Calculate the sizes of each split\n",
        "# dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "# train_split_size = int(train_size * dataset_size)\n",
        "# val_split_size = int(val_size * dataset_size)\n",
        "# test_split_size = int(test_size * dataset_size)\n",
        "\n",
        "\n",
        "# # Shuffle the dataset\n",
        "# shuffled_dataset = dataset.shuffle(dataset_size, seed = 90055)\n",
        "\n",
        "# # Split the dataset\n",
        "# train_dataset = shuffled_dataset.take(train_split_size)\n",
        "# val_dataset = shuffled_dataset.skip(train_split_size).take(val_split_size)\n",
        "# test_dataset = shuffled_dataset.skip(train_split_size + val_split_size).take(test_split_size)\n",
        "\n",
        "\n",
        "# # npp_train_dataset = train_dataset\n",
        "# # npp_val_dataset = val_dataset\n",
        "# # npp_test_dataset = test_dataset\n",
        "\n",
        "\n",
        "# # train_dataset = train_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
        "# # val_dataset = val_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
        "# # test_dataset = test_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
        " \n",
        "# # Print the sizes of each split\n",
        "# print(\"Training dataset size:\", tf.data.experimental.cardinality(train_dataset).numpy())\n",
        "# print(\"Validation dataset size:\", tf.data.experimental.cardinality(val_dataset).numpy())\n",
        "# print(\"Test dataset size:\", tf.data.experimental.cardinality(test_dataset).numpy())\n"
      ],
      "metadata": {
        "id": "-vP_bMsjlAkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}